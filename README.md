### Data Modeling with Postgres for Song Play Analysis

A new startup, Sparkify, wants to analyze the data they have been collecting on
songs and user activity on their new music streaming app. Currently, the data resides in a directory of JSON logs on user activity and a directory with JSON
metadata on the songs in their app.

Sparkify wants to create a Postgres database with tables designed to optimize
queries on song play analysis and learn what songs users are listening to. They
want to extract the data from the JSON files, transform and load it into this
new database.

The following fact and dimension tables for a star schema have been defined
to optimize the queries for analysis:

**Fact Table**

1. `songplays` - records in log data associated with song plays i.e. records
with page `Nextpage`

  *songplay_id, start_time, user_id, level, song_id, artist_id,
  session_id, location, user_agent*

**Dimension Tables**

1. `users` - users in the app

  *user_id, first_name, last_name, gender, level*

2. `songs` - songs in music database

  *song_id, title, artist_id, year, duration*

3. `artists` - artists in music database

  *artist_id, name, location, latitude, longitude*

4. `time` - timestamps of records in songplays broken down into specific units

  *start_time, hour, day, week, month, year, weekday*

#### Data
`song_data`: A subset of the [Million Song Dataset](http://millionsongdataset.com/).
Each file is in JSON format and contains metadata about a song and its artist.

`log_data`: JSON log files generated by this [event simulator](http://millionsongdataset.com/) based on the songs in the dataset above.
These simulate activity logs from a music streaming app based on specified configurations.

To read the data using [pandas](https://pandas.pydata.org/) use:

```python
df = pd.read_json(filepath, lines=True)
```

#### Files

`create_tables.py`: Creates, connects and closes sparkifydb database. Creates tables in the database.

`etl.ipynb`: Organize, develop and document the ETL process for each table before
completing the `etl.py` file to load the entire dataset into the database.

`etl.py`: Process and load the entire dataset into each table.

`sql_queries.py`: Postgres SQL statements to drop, create, insert, and select data.

`test.ipynb`: To confirm the creation of tables with correct columns and data.

#### Usage

[Postgres](https://www.postgresql.org/) installation and configuration required.

Libraries necessary:
```python
os
glob
psycopg2
pandas
```

On the command line run:
```python
> python create_tables.py
> python etl.py
```

Test table creation and data inserts using `test.ipynb`.
